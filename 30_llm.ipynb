{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<a href=\"https://colab.research.google.com/github/hdakhli/mlops-2024/blob/main/30_llm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d0cbbae3e38c51a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#\n",
    "## Word Embeddings Avec Word2Verc\n",
    "https://tedboy.github.io/nlps/generated/generated/gensim.models.Word2Vec.html"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "828a0b4f0962e22e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "!pip install gensim\n",
    "\n",
    "# Download EBOOK ALICE'S ADVENTURES IN WONDERLAND\n",
    "# https://www.gutenberg.org/files/11/11-0.txt"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8cf1fa2a7ea0c535"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Lire le fichier 'alice.txt'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ae46de51f1ebbeb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remplacer les retours à la ligne \\n par des espaces"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd3cc866a569d66c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Extraire chaque phrase et les mots qui la compose en un tableau de tableau à l'aide des fonctions sent_tokenize et word_tokenize de nltk.tokenize"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83d2e05ebc1405b2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Créer 2 modèles: CBOW et Skip Gram à partir de ce tableau. cf doc: gensim.models.Word2Vec"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a2e8606f3e9d682"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Comparer les mots suivants avec les 2 modelès : \n",
    "# 'alice' vs 'wonderland'\n",
    "# 'alice' vs 'machines'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3376a3d736522423"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3f9e71f777abcef6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2eacf360bd9f38f7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b6131519c8b43939"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#\n",
    "#\n",
    "## LLM\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e04b08c39eea43bf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Download: https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF/blob/main/llama-2-7b-chat.Q2_K.gguf\n",
    "!curl -O https: // huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF/blob/main/llama-2-7b-chat.Q2_K.gguf"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f00cb9390a0d80f7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "# Put the location of to the GGUF model that you've download from HuggingFace here\n",
    "model_path = \"llama-2-7b-chat.Q2_K.gguf\"\n",
    "\n",
    "# Create a model\n",
    "model = Llama(model_path=model_path)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32b3a74457e6c622"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Prompt creation\n",
    "system_message = \"Vous êtes un assistant utile. Vous êtes là pour répondre uniquement à ma question, répondez uniquement en Français, répondez uniquement à la dernière question\"\n",
    "user_message = \"Quelle est la capitale de la France ?\"\n",
    "\n",
    "prompt = f\"\"\"<s>[INST] <<SYS>>\n",
    "{system_message}\n",
    "<</SYS>>\n",
    "{user_message} [/INST]\"\"\"\n",
    "\n",
    "# Model parameters\n",
    "max_tokens = 50\n",
    "\n",
    "# Run the model\n",
    "output = model(prompt, max_tokens=max_tokens, echo=True)\n",
    "\n",
    "# Print the model output\n",
    "response = output['choices'][0]['text'].split(\"[/INST]\")[1]\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30ff6118b6cf97bd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "user_message = response + \", Et celle de l'Espagne ? \"\n",
    "prompt = f\"\"\"<s>[INST] <<SYS>>\n",
    "{system_message}\n",
    "<</SYS>>\n",
    "{user_message} [/INST]\"\"\"\n",
    "\n",
    "output = model(prompt, max_tokens=max_tokens, echo=True)\n",
    "response = output['choices'][0]['text'].split(\"[/INST]\")[1]\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ddf6b2c73feeb2a2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Que remarquez-vous ?\n",
    "# Comment pourriez-vous résoudre le problème ?\n",
    "# Exposer ce model en API REST avec FastAPI et Uvicorn"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ad75aa06671e005"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LangChain"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8a95f47d91247b6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install langchain-core langchain-community"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4780bd8a11d43fc4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# https://python.langchain.com/v0.1/docs/integrations/llms/llamacpp/#installation-with-windows"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e9f9e4f109979e12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"llama-2-7b-chat.Q2_K.gguf\",\n",
    "    # temperature=0.75,\n",
    "    # max_tokens=2000,\n",
    "    # top_p=1,\n",
    "    # callback_manager=callback_manager,\n",
    "    verbose=True,  # Verbose is required to pass to the callback manager\n",
    ")\n",
    "\n",
    "# Prompt\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer only the last question in French.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "\n",
    "def filter_messages(messages, k=10):\n",
    "    return messages[-k:]\n",
    "\n",
    "\n",
    "chain = (\n",
    "        RunnablePassthrough.assign(messages=lambda x: filter_messages(x[\"messages\"]))\n",
    "        | prompt\n",
    "        | llm\n",
    "        | output_parser\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c7283cb2e56a36ad"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "messages = []"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32cc8471f2694870"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "messages.append(HumanMessage(content=\"Quelle est la capitale de la France?\"))\n",
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": messages\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response)\n",
    "\n",
    "messages.append(AIMessage(content=response))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "572c6241a136a77"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "messages"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce5aed01f230dce4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "messages.append(HumanMessage(content=\"Et celle de l'Espagne ?\"))\n",
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": messages\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response)\n",
    "\n",
    "messages.append(AIMessage(content=response))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "721a389708180ae9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "messages"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f5d932d90dfc9636"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Faites en sorte que le modèle prenne en compte l'historique de la conversation\n",
    "# Exposer le model en API à l'aide de fast api et langserve"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c9a06edee44fd2f8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "794da630bbb0011c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#\n",
    "#\n",
    "#\n",
    "### Une autre alternarive\n",
    "#\n",
    "Télécharger et installer ollama: https://ollama.com/download\n",
    "\n",
    "En ligne de commande, télécharger le model:\n",
    "```shell\n",
    "ollama pull llama2\n",
    "```\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a31eb58ee40147e6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = Ollama(model=\"llama2\")\n",
    "\n",
    "# Prompt\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Vous êtes un assistant utile. Vous êtes là pour répondre à mes questions {input}.\",\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | llm | output_parser"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c52e13ffed4f2c4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "llm.invoke(\"Quelle est la capitale de la France ?\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ef193bf10c958ce"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Faites en sorte que le modèle prenne en compte l'historique de la conversation\n",
    "# Exposer le model en API à l'aide de fast api et langserve"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26de7579c280b512"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
